from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.python_operator import BranchPythonOperator
from datetime import datetime, timedelta


default_args = {
    'owner': 'thomas',
    'start_date': datetime(2023, 2, 10),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'dagPM',
    default_args=default_args,
    description='dag PM',
    schedule_interval=timedelta(days=1)
)

# Set up connection parameters from environment variables
def create_dw(**kwargs):
    import psycopg2
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_1"),
        user=os.getenv("DB_USER_1"),
        password=os.getenv("DB_PASSWORD_1")
    )
    cur = conn.cursor()
    # Creating tables
    cur.execute("""CREATE TABLE IF NOT EXISTS tweets(
                    id_tweet   INT PRIMARY KEY,
                    id_author BIGINT NOT NULL,
                    sentiment VARCHAR(8) NOT NULL
                    )""")
    cur.execute("""CREATE TABLE IF NOT EXISTS tweets_topics(
                    id_tweet INT,
                    topic VARCHAR(150),
                    PRIMARY KEY(id_tweet, topic),
                    CONSTRAINT fk_tweet
                        FOREIGN KEY(id_tweet)
                            REFERENCES tweets(id_tweet)
                    )""")
    cur.execute("""CREATE TABLE IF NOT EXISTS tweets_hashtags(
                    id_tweet INT,
                    hashtag VARCHAR(50),
                    PRIMARY KEY(id_tweet, hashtag),
                    CONSTRAINT fk_tweet
                        FOREIGN KEY(id_tweet)
                            REFERENCES tweets(id_tweet)
                    )""")
    conn.commit()
    conn.close()
    
#Loading the datalake
def init_dl(**kwargs):
    import psycopg2
    from psycopg2.extras import Json
    import json
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_2"),
        user=os.getenv("DB_USER_2"),
        password=os.getenv("DB_PASSWORD_2")
    )
    cur = conn.cursor()
    # Creating the table in the database if it does not exist
    cur.execute("""CREATE TABLE IF NOT EXISTS t_json(
                    id   INT    GENERATED BY DEFAULT AS IDENTITY,
                    c_json   json)""")
    cur.execute("""CREATE UNIQUE INDEX IF NOT EXISTS on_id_tweets ON t_json( (c_json->>'id') ) ;""")
    # Loading the JSON file
    my_file = open("/home/tr/airflow/dags/file/versailles_tweets_100.json", encoding="utf-8")
    data = json.load(my_file)
    # Inserting all tuples with automatically generated keys
    for index in range(len(data)):
        d = data[index]
        cur.execute("""INSERT INTO t_json (c_json) VALUES (%s) ON CONFLICT DO NOTHING""", [Json(d)])
    conn.commit()
    conn.close()  
  
def extract_from_db(**context):
    import psycopg2
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_2"),
        user=os.getenv("DB_USER_2"),
        password=os.getenv("DB_PASSWORD_2")
    )
    cur = conn.cursor()
    cur.execute("""SELECT * FROM t_json""")
    result = cur.fetchall()
    context['ti'].xcom_push(key='result', value=result)

# Function to retrieve the author ID of a tweet
def author_id(id_tweet):
    import psycopg2
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_1"),
        user=os.getenv("DB_USER_1"),
        password=os.getenv("DB_PASSWORD_1")
    )
    cur = conn.cursor()
    cur.execute("""SELECT c_json
                    FROM t_json
                    WHERE id = (%s)""", [id_tweet])
    row = cur.fetchone()
    conn.commit()
    conn.close()
    dic = row[0]
    return dic['author_id']

# Function to perform sentiment analysis on a tweet
def sentiment_analysis(id_tweet):
    import psycopg2
    from textblob import TextBlob
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_1"),
        user=os.getenv("DB_USER_1"),
        password=os.getenv("DB_PASSWORD_1")
    )
    cur = conn.cursor()
    cur.execute("""SELECT c_json
                        FROM t_json
                        WHERE id = (%s)""", [id_tweet])
    row = cur.fetchone()
    conn.commit()
    conn.close()
    dic = row[0]
    tweet = TextBlob(dic['text'])
    if tweet.sentiment.polarity >= 0:
        return "positive"
    return "negative

# Function to extract hashtags from a tweet
def hashtag_extraction(id_tweet):
    import psycopg2
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_1"),
        user=os.getenv("DB_USER_1"),
        password=os.getenv("DB_PASSWORD_1")
    )
    cur = conn.cursor()
    cur.execute("""SELECT c_json
                        FROM t_json
                        WHERE id = (%s)""", [id_tweet])
    row = cur.fetchone()
    conn.commit()
    conn.close()
    dic = row[0]
    list_hashtags = []
    try:
        d = dic['entities']['hashtags']
        for index in range(len(d)):
            hashtag = d[index]['tag']
            list_hashtags.append(hashtag)
    except KeyError:
        pass
    return list_hashtags


def topic_id(id_tweet):
    import psycopg2
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_1"),
        user=os.getenv("DB_USER_1"),
        password=os.getenv("DB_PASSWORD_1")
    )
    cur = conn.cursor()
    cur.execute("""SELECT c_json
                        FROM t_json
                        WHERE id = (%s)""", [id_tweet])
    row = cur.fetchone()
    conn.commit()
    conn.close()
    dic = row[0]
    list_topics = []
    try:
        d = dic['context_annotations']
        for index in range(len(d)):
            topic = d[index]['entity']['name']
            list_topics.append(topic)
    except KeyError:
        pass
    return list_topics

def all_authors(**context):
    result = context['ti'].xcom_pull(task_ids='extract_from_db', key='result')
    dict_all_authors = {}
    for row in result:
        id = row[0]
        author = author_id(id)
        dict_all_authors[id] = author
    print("########### ",dict_all_authors)
    context['ti'].xcom_push(key='all_authors', value=dict_all_authors)

def all_sentiment(**context):
    result = context['ti'].xcom_pull(task_ids='extract_from_db', key='result')
    dict_all_sentiment = {}
    for row in result:
        id = row[0]
        sentiment = sentiment_analysis(id)
        dict_all_sentiment[id] = sentiment
    print("########### ",dict_all_sentiment)
    context['ti'].xcom_push(key='all_sentiment', value=dict_all_sentiment)

def all_hashtags(**context):
    result = context['ti'].xcom_pull(task_ids='extract_from_db', key='result')
    dict_all_hashtags = {}
    for row in result:
        id = row[0]
        hashtags = hashtag_extraction(id)
        if hashtags:
            dict_all_hashtags[id] = hashtags 
    print("########### ",dict_all_hashtags)
    context['ti'].xcom_push(key='all_hashtags', value=dict_all_hashtags)

def all_topics(**context):
    result = context['ti'].xcom_pull(task_ids='extract_from_db', key='result')
    dict_all_topics = {}
    for row in result:
        id = row[0]
        topics = topic_id(id)
        if topics:
            dict_all_topics[id] = topics 
    print("########### ",dict_all_topics)
    context['ti'].xcom_push(key='all_topics', value=dict_all_topics)



def insert_dw(**context):
    import psycopg2
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_1"),
        user=os.getenv("DB_USER_1"),
        password=os.getenv("DB_PASSWORD_1")
    )
    conn.autocommit = True
    cursor_dw = conn.cursor()
    # Inserting tuples and/or updating the database
    authors = context['ti'].xcom_pull(task_ids='all_authors', key='all_authors')
    sentiment = context['ti'].xcom_pull(task_ids='all_sentiment', key='all_sentiment')
    hashtags = context['ti'].xcom_pull(task_ids='all_hashtags', key='all_hashtags')
    topics = context['ti'].xcom_pull(task_ids='all_topics', key='all_topics')

    for id in authors.keys():
        cursor_dw.execute("""INSERT INTO tweets(id_tweet, id_author, sentiment)
                        VALUES (%s, %s, %s)
                        ON CONFLICT DO NOTHING""", (id, authors[id], sentiment[id]))

    for id in hashtags.keys():
        for h in hashtags[id]: 
            cursor_dw.execute("""INSERT INTO tweets_hashtags(id_tweet, hashtag) 
                                VALUES (%s, %s)
                                ON CONFLICT DO NOTHING
                            """, (id, h))
                 
    for id in topics.keys():
        for t in topics[id]:
            cursor_dw.execute("""INSERT INTO tweets_topics(id_tweet, topic) 
                                    VALUES (%s, %s)
                                    ON CONFLICT DO NOTHING
                                """, (id, t))

    conn.close()
    


# Function to count publications by user
def publication_by_user(**kwargs):
    import psycopg2
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_2"),
        user=os.getenv("DB_USER_2"),
        password=os.getenv("DB_PASSWORD_2")
    )
    cur = conn.cursor()
    cur.execute("""SELECT id_author, COUNT(*) AS nb_p
                    FROM tweets
                    GROUP BY id_author
                    """)
    result = cur.fetchall()
    conn.commit()
    conn.close()
    dic_users = {}
    try:
        for row in result:
            users = str(row[0])
            nb_p = row[1]
            dic_users["U" + users] = nb_p
            print(dic_users)
    except KeyError:
        pass
    return dic_users

# Function to count publications by hashtag
def publication_by_hashtag(**kwargs):
    import psycopg2
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_2"),
        user=os.getenv("DB_USER_2"),
        password=os.getenv("DB_PASSWORD_2")
    )
    cur = conn.cursor()
    cur.execute("""SELECT hashtag, COUNT(*) AS nb_p
                    FROM tweets_hashtags
                    GROUP BY hashtag
                    """)
    result = cur.fetchall()
    conn.commit()
    conn.close()
    dic_hashtags = {}
    try:
        for row in result:
            hashtags = row[0]
            nb_p = row[1]
            dic_hashtags[hashtags] = nb_p
            print(dic_hashtags)
    except KeyError:
        pass
    return dic_hashtags

# Function to count publications by topic
def publication_by_topic(**kwargs):
    import psycopg2
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_DATABASE_2"),
        user=os.getenv("DB_USER_2"),
        password=os.getenv("DB_PASSWORD_2")
    )
    cur = conn.cursor()
    cur.execute("""SELECT topic, COUNT(*) AS nb_p
                    FROM tweets_topics
                    GROUP BY topic
                    """)
    result = cur.fetchall()
    conn.commit()
    conn.close()
    dic_topics = {}
    try:
        for row in result:
            topics = row[0]
            topics = topics.replace(" ", "_")
            topics = topics.replace("&", "and")
            nb_p = row[1]
            dic_topics[topics] = nb_p
            print(dic_topics)
    except KeyError:
        pass
    return dic_topics


init_dl_task = PythonOperator(
    task_id='init_dl',
    python_callable=init_dl,
    dag=dag
)

create_dw_task = PythonOperator(
    task_id='create_dw',
    python_callable=create_dw,
    dag=dag
)

extract_task = PythonOperator(
    task_id='extract_from_db',
    python_callable=extract_from_db,
    provide_context=True,
    dag=dag
)



insert_dw_task = PythonOperator(
    task_id='insert_dw',
    python_callable=insert_dw,
    provide_context=True,
    dag=dag
)



all_authors_task = PythonOperator(
    task_id='all_authors',
    python_callable=all_authors,
    provide_context=True,
    dag=dag
)


all_sentiment_task = PythonOperator(
    task_id='all_sentiment',
    python_callable=all_sentiment,
    provide_context=True,
    dag=dag
)

all_hashtags_task = PythonOperator(
    task_id='all_hashtags',
    python_callable=all_hashtags,
    provide_context=True,
    dag=dag
)

all_topics_task = PythonOperator(
    task_id='all_topics',
    python_callable=all_topics,
    provide_context=True,
    dag=dag
)


publication_by_user_task = PythonOperator(
    task_id='publication_by_user_task',
    python_callable=publication_by_user,
    dag=dag
)

publication_by_hashtag_task = PythonOperator(
    task_id='publication_by_hashtag',
    python_callable=publication_by_hashtag,
    dag=dag
)

publication_by_topic_task = PythonOperator(
    task_id='publication_by_topic',
    python_callable=publication_by_topic,
    dag=dag
)

init_dl_task >> create_dw_task 
create_dw_task >> extract_task >> all_authors_task >> all_sentiment_task >> all_hashtags_task >> all_topics_task >> insert_dw_task 

insert_dw_task >>  publication_by_user_task
insert_dw_task >> publication_by_hashtag_task
insert_dw_task >> publication_by_topic_task
